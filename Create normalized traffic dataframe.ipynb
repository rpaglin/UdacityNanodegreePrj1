{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notebook 1 - Mobile traffic data\n",
    "Streaming of high hype sport events (particularly football matches) is creating some additional stress on network capacity.\n",
    "Topic became even more of interest when a pure internet broadcaster acquired the right for serie A matches for next season. \n",
    "The project is about some numerical investigation on the relationship between traffic growth on a mobile network in Italy and the scheduling of football matches involving italian teams. Specifically, investigation will address the following questions:\n",
    "\n",
    "- Can we identify a significative numerical correlation between mobile traffic peaks and footbal matches schedule?\n",
    "- If yes, can we add insigth for classes of football event parameters showing higher correlations (e.g. number of contemporary matches, hype of a single match, types of broadcaster)? \n",
    "- Can the information above be used to tune traffic growth forecasts on a mobile network?\n",
    "\n",
    "The activity is intended as an investigation exercise, so data actually used, though obtained from real measurement, will first be normalized to range between artificially set bounds.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datasets\n",
    "\n",
    "The notebook will leverage on following data:\n",
    "\n",
    "- Some real traffic measurement from a mobile network on a generic geographical area in Italy over about 2 years. Data are normalized in a way that reduces all used kpi to range between 1 and 2. Only normalized and region wide values are shown.\n",
    "- Scheduling of major footbal competitions involving italian teams (I considered 'Serie A' and 'Champions League')\n",
    "- Information about the broadcaster distribuiting the matches (internet only or pay tv+internet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notebook organization\n",
    "Activity is organized into different jupyter notebooks:\n",
    "- The first notebook (the present one) is about preparing reading traffic data and adding normalization. As actual starting data will not be shared, this notebook is only intended for reference\n",
    "- The second notebook will analize and prepare normalized data for actual usage \n",
    "- Third notebook reads, manipulates and prepare sport events dataframe\n",
    "- Fourth notebook  merges all collected and preprocessed data and investigate relationship to try to answer the target questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rpaglin\\OneDrive - Vodafone Group\\Desktop\\Nanodegree prj1 data\n"
     ]
    }
   ],
   "source": [
    "cd \"C:\\\\Users\\\\rpaglin\\\\OneDrive - Vodafone Group\\\\Desktop\\\\Nanodegree prj1 data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readxpottercsvfiles(path, filenames):\n",
    "    ''' The function read a list of csv file with known format into a dataframe. Each CSV will consiste of 6 columns, \n",
    "    5 of them mainatined in the df:\n",
    "    'DATE': date in yyyy-mm-dd format\n",
    "    'TIME': Time of measurement as hh:mm:ss. Only hours are available in the used datafiles, so min and sec wil be dropped  \n",
    "    'RESOURCE NAME': Network element that collected the measurement\n",
    "    'KPI NAME': type of measurement\n",
    "    'VALUE': actual measurement\n",
    "    INPUT\n",
    "    path: directory to be searched for files\n",
    "    filenames: python list of filenames\n",
    "    OUTPUT\n",
    "    The function return a dataframe consolidating info from all files read.\n",
    "    DATE field is translated into a datetime, while TIME field is memorized with an integer representing hour  \n",
    "    '''\n",
    "    df = pd.read_csv(path+filenames[0],thousands=',',usecols=['RESOURCE NAME','KPI NAME', 'DATE', 'TIME','VALUE'])\n",
    "    for fn in filenames[1:]:\n",
    "        df_tmp = pd.read_csv(path+fn, thousands=',',usecols=['RESOURCE NAME','KPI NAME', 'DATE', 'TIME','VALUE'])\n",
    "        df=pd.concat([df,df_tmp])\n",
    "    df.drop_duplicates(['DATE','TIME','KPI NAME','RESOURCE NAME'], inplace=True)\n",
    "    df['DATE']=df['DATE'].apply(lambda x: pd.datetime(int(x[0:4]),int(x[5:7]),int(x[8:10])))\n",
    "    df['TIME']=df['TIME'].apply(lambda x: int(x[0:2]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_values(df, col, w_0,w_m):\n",
    "    ''' Purpose of this function is to partially hyde actual network data, forcing measurements \n",
    "    in the analized period to range from artificially set min and max values.\n",
    "    As the purpose of the activity is to exercise on a methodology, in the follwoing we will consider \n",
    "     normalized data as valid values and we will not bother on the effect of this normalization on results\n",
    "     \n",
    "    INPUT\n",
    "    df: a dataframe containing a column to be normalized\n",
    "    col: the column identifier\n",
    "    w_0: Artificial value that will be assigned to the max measurement taken in day 1\n",
    "    w_m: Artificial value that will be assigned to the max measurement taken in last full day\n",
    "    OUTPUT\n",
    "    The function return df dataframe with the column 'col' normalized around w_0 and w_m\n",
    "    Note that the approach taken will not force outlyers to be in the expected range\n",
    "    '''\n",
    "    df1=df.groupby('DATE', axis=0).max()\n",
    "    v_m=df1[col].iloc[-2] #max on last full day\n",
    "    v_0=df1[col].iloc[0]  #max on first full day\n",
    "    delta_v=v_m-v_0+0.\n",
    "    delta_w=w_m-w_0+0.\n",
    "    delta_ratio=delta_w/delta_v\n",
    "    df[col]=w_m-(v_m-df[col])*delta_ratio\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Throughput Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='C:\\\\Users\\\\rpaglin\\\\OneDrive - Vodafone Group\\\\Desktop\\\\Nanodegree prj1 data\\\\Xpotter Thr\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames=['report_rpaglin_thr19Q1.csv','report_rpaglin_thr19Q2.csv','report_rpaglin_thr19Q3.csv','report_rpaglin_thr19Q4.csv']\n",
    "filenames+=['report_rpaglin_thr20Q1.csv','report_rpaglin_thr20Q2.csv','report_rpaglin_thr20Q3.csv']\n",
    "filenames+=['report_rpaglin_thr20Q4.csv','report_rpaglin_thr21Q1.csv','report_rpaglin_thr21Q2.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "thr_df=readxpottercsvfiles(path, filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 397153 entries, 0 to 27607\n",
      "Data columns (total 5 columns):\n",
      "RESOURCE NAME    397153 non-null object\n",
      "KPI NAME         397153 non-null object\n",
      "DATE             397153 non-null datetime64[ns]\n",
      "TIME             397153 non-null int64\n",
      "VALUE            395291 non-null float64\n",
      "dtypes: datetime64[ns](1), float64(1), int64(1), object(2)\n",
      "memory usage: 18.2+ MB\n"
     ]
    }
   ],
   "source": [
    "thr_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "thr_df=thr_df.groupby(by=['DATE', 'TIME'], axis=0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "thr_df=normalize_values(thr_df,'VALUE',1.,2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VALUE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-01</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-02</th>\n",
       "      <td>0.995058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-03</th>\n",
       "      <td>0.995295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-04</th>\n",
       "      <td>0.967564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-05</th>\n",
       "      <td>0.925946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               VALUE\n",
       "DATE                \n",
       "2019-01-01  1.000000\n",
       "2019-01-02  0.995058\n",
       "2019-01-03  0.995295\n",
       "2019-01-04  0.967564\n",
       "2019-01-05  0.925946"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thr_df.groupby('DATE').max().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connection data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='C:\\\\Users\\\\rpaglin\\\\OneDrive - Vodafone Group\\\\Desktop\\\\Nanodegree prj1 data\\\\Xpotter Cn\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames=['report_rpaglin_cn1_19Q2.csv','report_rpaglin_cn1_19Q3.csv','report_rpaglin_cn1_19Q4.csv']\n",
    "filenames+=['report_rpaglin_cn1_20Q1.csv','report_rpaglin_cn1_20Q2.csv','report_rpaglin_cn1_20Q3.csv']\n",
    "filenames+=['report_rpaglin_cn1_20Q4.csv','report_rpaglin_cn1_21Q1.csv','report_rpaglin_cn1_21Q2.csv']\n",
    "filenames+=['report_rpaglin_cn2_19Q1.csv']\n",
    "filenames+=['report_rpaglin_cn2_19Q2.csv','report_rpaglin_cn2_19Q3.csv','report_rpaglin_cn2_19Q4.csv']\n",
    "filenames+=['report_rpaglin_cn2_20Q1.csv','report_rpaglin_cn2_20Q2.csv','report_rpaglin_cn2_20Q3.csv']\n",
    "filenames+=['report_rpaglin_cn2_20Q4.csv','report_rpaglin_cn2_21Q1.csv','report_rpaglin_cn2_21Q2.csv']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>VALUE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <th>TIME</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">2019-01-01</th>\n",
       "      <th>0</th>\n",
       "      <td>0.949642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.920442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.885188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    VALUE\n",
       "DATE       TIME          \n",
       "2019-01-01 0     0.949642\n",
       "           1     0.920442\n",
       "           2     0.885188"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn_df=readxpottercsvfiles(path, filenames)\n",
    "cn_df=cn_df.groupby(by=['DATE', 'TIME'], axis=0).sum()\n",
    "cn_df=normalize_values(cn_df,'VALUE',1.,2.)\n",
    "cn_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging info into a single dataframe and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21036 entries, 0 to 21035\n",
      "Data columns (total 4 columns):\n",
      "DATE           21036 non-null datetime64[ns]\n",
      "TIME           21036 non-null int64\n",
      "Thr (Norm)     20923 non-null float64\n",
      "Conn (Norm)    21036 non-null float64\n",
      "dtypes: datetime64[ns](1), float64(2), int64(1)\n",
      "memory usage: 657.5 KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>TIME</th>\n",
       "      <th>Thr (Norm)</th>\n",
       "      <th>Conn (Norm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0.951451</td>\n",
       "      <td>0.949642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0.839571</td>\n",
       "      <td>0.920442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>0.710031</td>\n",
       "      <td>0.885188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>3</td>\n",
       "      <td>0.589674</td>\n",
       "      <td>0.858900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>4</td>\n",
       "      <td>0.497652</td>\n",
       "      <td>0.844702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        DATE  TIME  Thr (Norm)  Conn (Norm)\n",
       "0 2019-01-01     0    0.951451     0.949642\n",
       "1 2019-01-01     1    0.839571     0.920442\n",
       "2 2019-01-01     2    0.710031     0.885188\n",
       "3 2019-01-01     3    0.589674     0.858900\n",
       "4 2019-01-01     4    0.497652     0.844702"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thr_df.rename(columns={'VALUE':'Thr (Norm)'},inplace=True)\n",
    "cn_df.rename(columns={'VALUE':'Conn (Norm)'},inplace=True)\n",
    "nw_df=pd.concat([thr_df, cn_df], axis=1)\n",
    "nw_df.reset_index(inplace=True)\n",
    "nw_df.info()\n",
    "nw_df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "nw_df.to_csv('C:\\\\Users\\\\rpaglin\\\\OneDrive - Vodafone Group\\\\Desktop\\\\Nanodegree prj1 data\\\\nw_df.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
